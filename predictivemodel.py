# -*- coding: utf-8 -*-
"""PredictiveModel.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1IG6Yg9QWC1Z-hiYEBeaJwfoPzkMxp1I8

# Building a Predictive Model

# Import the Necessary Python Libraries
"""

import pandas as pd
import numpy as np
import seaborn as sns
import matplotlib.pyplot as plt
from sklearn import model_selection
from sklearn.datasets import make_blobs
from sklearn.neighbors import KNeighborsClassifier
from sklearn.linear_model import LogisticRegression
from sklearn.model_selection import train_test_split

"""
#Reading the Dataset"""

from google.colab import drive
drive.mount('/content/drive')

path = '/content/drive/MyDrive/DataScience/donor_data.csv'
df = pd.read_csv(path)
df.head(5)

df.isnull() #Check all null values in the dataset

"""****"""

df.isnull().sum() #Explore how many null values for each column

"""#Solution - 2 Remove rows with empty values

"""

Dataset_Modified = df.fillna(0)

Dataset_Modified.isnull().sum() #Verify no longer have any null value

"""#Exploring Dataset


"""

Dataset_Modified.info()

"""****"""

Dataset_Modified.shape #The shape function displays the number of records and columns:

Dataset_Modified.describe()#The describe() function summarizes the dataset’s statistical properties, such as count, mean, min, and max:

"""#Feature Selection

In this step, we choose several features that contribute most to the target output. So, instead of training the model using every column in our dataset, we select only those that have the strongest relationship with the predicted variable.


"""

from sklearn.feature_selection import SelectKBest #Start by importing the SelectKBest library:
from sklearn.feature_selection import chi2

"""After, define X & Y:"""

X= Dataset_Modified.iloc[:,2:49]  # all features
Y= Dataset_Modified.iloc[:,1]   #target output (target_B)

"""Select the top 4 features:"""

X=  Dataset_Modified[['DONOR_AGE','INCOME_GROUP','LIFETIME_GIFT_COUNT','WEALTH_RATING']]  #the top 4 features
Y=  Dataset_Modified[['TARGET_B']]  #the target output

"""Second, split the dataset into train and test:"""

X_train, X_test, y_train, y_test = train_test_split(X, Y, random_state = 0)

"""**Step 4: KNN Classifier Implementation**

After that, I’ll build a kNN classifier object. I develop classifier with k values of 5 to demonstrate the relevance of the k value. The models are then trained using a train set. The k value is chosen using the n_neighbors argument. 
"""

knn5 = KNeighborsClassifier(n_neighbors = 5)

"""**Step 5: Predictions for the KNN Classifiers**

Then, in the test set, we forecast the target values and compare them to the actual values.
"""

knn5.fit(X_train, y_train)
y_pred_5 = knn5.predict(X_test)

"""**Step 6:Evaluate the Model’s Performance**

As a final step, we’ll evaluate how well our Python model performed predictive analytics by running a classification report and a ROC curve.
"""

from sklearn import metrics
from sklearn.metrics import classification_report
print("Accuracy :",metrics.accuracy_score(y_test, y_pred_5))
print("Recall   :",metrics.recall_score(y_test, y_pred_5, zero_division=1))
print("Precision:",metrics.precision_score(y_test, y_pred_5, zero_division=1))
print("CL Report:",metrics.classification_report(y_test, y_pred_5, zero_division=1))